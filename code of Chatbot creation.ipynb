{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot with Custom Data\n",
    "\n",
    "This notebook guides you through the creation of a chatbot tailored to your specific data needs. We utilize Elastic Search as our vector storage solution and incorporate the \"meta-llama/llama-2-13b-chat\" model from WatsonX's Large Language Models (LLM). The `langchain` library plays a crucial role in this process, aiding in tasks like chunking documents, indexing data in Elastic Search, managing conversation chains with memory buffers, and crafting prompt templates.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **PDF Content Processing**: When users upload PDF files, the notebook extracts the text, segments it into manageable chunks, and indexes these chunks in Elastic Search using an appropriate `elser` model.\n",
    "- **Data-Driven Query Handling**: Users can pose questions to the chatbot, which searches the indexed data for relevant answers.\n",
    "- **Integrating Elastic Search and WatsonX LLMs**: We leverage `langchain`'s capabilities to link Elastic Search indexing with WatsonX's LLMs, enabling a seamless conversational experience with memory and retrieval functionalities.\n",
    "- **Hallucination Check**: The notebook includes a mechanism to detect and correct any hallucinations or inaccuracies in the LLM's responses.\n",
    "\n",
    "### Prerequisites for Running the Notebook:\n",
    "\n",
    "\n",
    "1. **Library Requirements**: Confirm that you have installed all libraries specified in the `requirements.txt` file.\n",
    "2. **Elastic Search ELSER Model Setup**: Implement an ELSER model within your Elastic Search instance. Refer to the Elastic documentation for setup details: [Elastic Machine Learning: ELSER](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html).\n",
    "3. **Environment Configuration**: A `.env` file is required, containing critical configuration details:\n",
    "\n",
    "   - **`elastic_search_url`**: This URL connects you to your Elastic Search instance, a search engine built on the Lucene library, offering distributed, multitenant capabilities for full-text search with a web interface and JSON document handling. The `elastic_search_url` serves as your point of interaction for tasks like data indexing and querying.\n",
    "   - **`elastic_search_api_key`**: A key for secure access to your Elastic Search instance, this API key is essential for authentication and authorization, ensuring that only permitted users and applications interact with your Elastic Search server.\n",
    "   - **`WATSONX_APIKEY`**: Your access key for IBM's WatsonX services, this API key is used for authenticating requests to WatsonX's AI and cognitive computing services. Acquire your WatsonX.ai URL and API key by following these instructions: [IBM Cloud: API Keys](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key) and [WatsonX as a Service Documentation](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=library-credentials).\n",
    "   - **`WATSONX_URL`**: The primary access point for WatsonX's API, this URL is where you connect to utilize WatsonX's diverse AI functionalities.\n",
    "   - **`WATSONX_Project_ID`**: A unique identifier for your project in the WatsonX environment, this ID helps manage and organize resources like datasets and AI models within your specified WatsonX project.Obtain a project ID for WatsonX AI, essential for project management within WatsonX. Guides for this can be found here: [Creating a WatsonX Project](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=projects-creating-project), [Finding Your Project ID in IBM Cloud](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-project-id.html?context=wx).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-using-elasticsearch-WatsonX/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import PyPDF2\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894648d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f2c32",
   "metadata": {},
   "source": [
    "### User Inputs \n",
    "User can specify/update below inputs as per their needs. \n",
    "\n",
    "\n",
    "`es_model_id`: This refers to the unique identifier of the Elastic Search (ES) model that's being used. Elastic Search models, like the ELSER model, are deployed within the Elastic Search environment to perform specific tasks such as text analysis, natural language processing, or vector search. The es_model_id helps in identifying and referencing the specific model deployed in your Elastic Search instance.\n",
    "\n",
    "`index_name`: In the context of Elastic Search, an index_name denotes the name of the index where your data is stored. The index_name is used to specify which collection of documents you're querying or modifying in your Elastic Search operations. If the index is not already present, one with this name gets created during runtime.\n",
    "\n",
    "`llm_model_id`: This is the identifier for the Large Language Model (LLM) from WatsonX that you're using. WatsonX provides various AI models, including LLMs for different tasks like conversation, text completion, or language translation. The llm_model_id allows you to specify which of these models you want to interact with in your application.\n",
    "\n",
    "`wx_url`: This variable represents the URL for the WatsonX service. WatsonX, being a cloud-based service, can be accessed through its dedicated URL. This URL is used to make API requests, authenticate your application, and access the services provided by WatsonX, like their LLMs or other AI functionalities.\n",
    "\n",
    "`wx_project_id`: The wx_project_id is a unique identifier for a project within the WatsonX ecosystem. In WatsonX, a project is a workspace where you can organize resources, data, models, and other assets. Each project has a unique ID which is used to access and manage the resources within that specific project. This ID ensures that your interactions with the WatsonX API are scoped and managed within the right project context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b513f0bbb83996e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:06.281695Z",
     "start_time": "2024-01-07T10:24:06.277326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "es_model_id = '.elser_model_2_linux-x86_64'\n",
    "index_name = \"elser_index_vb_test_2\"\n",
    "llm_model_id = \"meta-llama/llama-2-13b-chat\" #\"ibm/granite-13b-chat-v2\"#\n",
    "wx_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "wx_project_id = os.environ[\"WATSONX_Project_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "### Enter your pdf file name below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57efcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs=[\"Industry accelerators - IBM Documentation.pdf\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare above documents and their metadata\n",
    "The prepare_docs function below processes a list of PDF documents by extracting text from each page and organizing it into two lists: one for the text content and another for the metadata (titles). It iterates through each page of each PDF, extracts the text, and forms a title using the PDF name and page number. The function returns these two lists, making it useful for indexing and referencing the content of multiple PDFs at a page level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "        for index, text in enumerate(pdf_reader.pages):\n",
    "            doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
    "                        'content': pdf_reader.pages[index].extract_text()}\n",
    "            docs.append(doc_page)\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921288be",
   "metadata": {},
   "source": [
    "### Step 2: Chunk the documents \n",
    "The get_text_chunks function takes text content and metadata as inputs and splits the content into smaller chunks. It uses a RecursiveCharacterTextSplitter configured with a specified chunk size (512 characters) and overlap (256 characters) for this purpose. The function processes the content, splitting it into passages while maintaining associated metadata. After splitting, it prints the total number of passages created and returns these split documents. This function is useful for breaking down large text into more manageable, indexed segments for easier processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e8895acb436e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:47:25.155847Z",
     "start_time": "2024-01-07T07:47:25.149629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Elastic Search \n",
    "The ingest_and_get_vector_store function initializes and populates an Elasticsearch vector store with provided document chunks (split_docs). It creates an ElasticsearchStore instance using environment variables for the Elastic Search URL, API key, index name, and a retrieval strategy based on a specified Elastic Search model ID (es_model_id). The function then ingests the split documents into this Elasticsearch store. After processing, it returns the populated vector_store, enabling the storage and retrieval of document vectors for efficient search and analysis. This function essentially sets up and populates an Elasticsearch-based vector store tailored for handling segmented document data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956e5ad8704a563f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:47:34.333240Z",
     "start_time": "2024-01-07T07:47:34.330872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ingest_and_get_vector_store(split_docs):\n",
    "    vector_store = ElasticsearchStore(\n",
    "                    es_cloud_id= os.environ[\"elastic_search_cloud_id\"],\n",
    "                    es_api_key=os.environ[\"elastic_search_api_key\"],\n",
    "                    index_name=index_name,\n",
    "                    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=es_model_id)\n",
    "                    )\n",
    "    documents = vector_store.from_documents(\n",
    "        split_docs,\n",
    "        es_cloud_id= os.environ[\"elastic_search_cloud_id\"],\n",
    "        es_api_key=os.environ[\"elastic_search_api_key\"],\n",
    "        index_name=index_name,\n",
    "        strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=es_model_id)\n",
    "    )\n",
    "    print(\"Documents indexed and vector Store returned\")\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain` function sets up a conversational chain for a chatbot using a vector store, a language model, and memory management. Key steps include:\n",
    "\n",
    "1. **Setting Up LLM Parameters**: Defines various generation parameters for the WatsonX Large Language Model (LLM), like decoding method, token limits, temperature, and selection criteria (Top-K, Top-P).\n",
    "\n",
    "2. **Initializing WatsonX LLM**: Creates a `WatsonxLLM` instance using the LLM model ID, WatsonX URL, project ID, the specified parameters, and an API key from the environment.\n",
    "\n",
    "3. **Creating a Retriever**: Transforms the provided `vector_store` into a retriever for fetching relevant documents.\n",
    "\n",
    "4. **Preparing a Prompt Template**: Utilizes a prompt template for structuring the queries sent to the LLM. User can create/update their own template below.\n",
    "\n",
    "5. **Setting Up Conversation Memory**: Implements a `ConversationBufferMemory` to manage chat history and output answers.\n",
    "\n",
    "6. **Building the Conversational Chain**: Constructs a `ConversationalRetrievalChain` by combining the LLM, retriever, prompt template, and memory. This chain also returns source documents alongside responses.\n",
    "\n",
    "The function ultimately returns this configured conversation chain, which is essential for handling and responding to user queries effectively in a chatbot, integrating both generative AI and information retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template =\"\"\"[INST]You are a helpful, respectful, and honest assistant. \n",
    "Always answer as helpfully as possible, while being safe. Be brief in your answers. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If you don\\\\'\\''t know the answer to a question, please do not share false information. \\n Answer with no more than 150 words, in 2 or 3 sentences. If you cannot base your answer on the given document, please state that you do not have an answer.\\n\\n{question} Answer with no more than 200 words. If you cannot base your answer on the given document, please state that you do not have an answer. do not include a question in your response. dont prompt to make select correct answers[/INST]\"\"\"\n",
    "\n",
    "template = \"\"\"[INST]\n",
    "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
    "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
    "- Do not fabricate information or include questions in your responses.\n",
    "- do not prompt to select answers. do not ask me questions\n",
    "\n",
    "{question}\n",
    "\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_conversation_chain(vector_store):\n",
    "    openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    retriever = vector_store.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=openai_llm,\n",
    "                           retriever=retriever,\n",
    "                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Detect Hallucination in the LLMs Response\n",
    "The `validate_answer_against_sources` function evaluates the reliability of a response by comparing it with source documents. It works as follows:\n",
    "\n",
    "1. **Model Initialization**: Utilizes the SentenceTransformer model 'all-MiniLM-L6-v2' to generate embeddings.\n",
    "\n",
    "2. **Threshold Setting**: Sets a similarity threshold (here, 0.5) to determine the acceptable level of similarity between the response and source documents.\n",
    "\n",
    "3. **Extracting Source Texts**: Gathers the content of the source documents.\n",
    "\n",
    "4. **Computing Embeddings**: Generates embeddings for both the response answer and the source texts.\n",
    "\n",
    "5. **Calculating Similarity**: Computes cosine similarity scores between the response answer's embedding and the embeddings of each source text.\n",
    "\n",
    "6. **Validity Check**: Checks if any of the similarity scores exceed the set threshold. If yes, it implies that the response is sufficiently similar to at least one of the source documents, suggesting its reliability, and returns `True`. If not, it returns `False`.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_answer_against_sources(response_answer, source_documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    similarity_threshold = 0.5  \n",
    "    source_texts = [doc.page_content for doc in source_documents]\n",
    "\n",
    "    answer_embedding = model.encode(response_answer, convert_to_tensor=True)\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
    "\n",
    "\n",
    "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
    "        return True  \n",
    "\n",
    "    return False  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = prepare_docs(pdf_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e99300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 3 passages\n"
     ]
    }
   ],
   "source": [
    "split_docs = get_text_chunks(content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f714b5f5df09e897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:13.506700Z",
     "start_time": "2024-01-07T10:24:12.199767Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexed and vector Store returned\n"
     ]
    }
   ],
   "source": [
    "vectorstore = ingest_and_get_vector_store(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79d503befc592a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:24.130465Z",
     "start_time": "2024-01-07T10:44:22.503570Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Chain created for the LLM using the vector store\n"
     ]
    }
   ],
   "source": [
    "conversation_chain=get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  what are industry accelerators?\n",
      "A:  I'm sorry, but that is not the correct definition of industry accelerators in the context of IBM Documentation. According to the provided information, industry accelerators are a set of artifacts provided by IBM that help address common business issues. They are designed to help solve specific business problems and typically include a sample project with instructions, data sets, notebooks, and models. They also include a business glossary that provides terms and categories for data governance.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"what are industry accelerators?\"\n",
    "response=conversation_chain({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d849",
   "metadata": {},
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n",
    "\n",
    "\n",
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "269d50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  where are they located?\n",
      "A:  I'm sorry, but I don't have enough information to provide an answer based on the provided document.\n",
      "\n",
      "Conversation Chain: \n",
      " {'question': 'where are they located?', 'chat_history': [HumanMessage(content='what are industry accelerators?'), AIMessage(content='Industry accelerators are a set of artifacts provided by IBM that help address common business issues. Each accelerator is designed to solve a specific business problem, such as preventing credit card fraud or optimizing the efficiency of a contact center. These accelerators include sample projects with instructions, data sets, Jupyter notebooks, models, and R Shiny applications. They also include a business glossary that consists of terms and categories for data governance. Industry accelerators are intended for data scientists or business analysts who analyze data and build models to solve business problems.'), HumanMessage(content='where can i find them?'), AIMessage(content=\"I'm sorry, but I don't have enough information to answer your question.\"), HumanMessage(content='where to find them?'), AIMessage(content=\"I'm sorry, but I cannot provide an answer based on the provided document.\"), HumanMessage(content='how many are there?'), AIMessage(content=\"I'm sorry, but I don't have enough information to provide an answer to your question.\"), HumanMessage(content='how to import them?'), AIMessage(content=\"I'm sorry, but I don't have enough information to answer your question.\"), HumanMessage(content='who are the audience?'), AIMessage(content=\"I'm sorry, but I don't have enough information to answer your question based on the provided document.\"), HumanMessage(content='who are the audience of industry accelerators?'), AIMessage(content=\"I'm sorry, but the document does not provide specific information about the audience of industry accelerators.\"), HumanMessage(content='what are industry accelerators?'), AIMessage(content=\"I'm sorry, but that is not the correct definition of industry accelerators in the context of IBM Documentation. According to the provided information, industry accelerators are a set of artifacts provided by IBM that help address common business issues. They are designed to help solve specific business problems and typically include a sample project with instructions, data sets, notebooks, and models. They also include a business glossary that provides terms and categories for data governance.\"), HumanMessage(content='where are they located?'), AIMessage(content=\"I'm sorry, but I don't have enough information to provide an answer based on the provided document.\")], 'answer': \"I'm sorry, but I don't have enough information to provide an answer based on the provided document.\", 'source_documents': [Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}), Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}), Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}), Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 1/3Industry accelerators\\nLast Updated: 2023-11-29\\nThe industry accelerators that are provided by IBM are a set of artifacts that help you address common\\nbusiness issues.\\nEach industry accelerator is designed to help you solve a speciﬁc business problem, whether it's\\npreventing credit card fraud in the banking industry or optimizing the efﬁciency of your contact center.\\nBrowse the Accelerators catalog for the Cloud Pak for Data industry accelerators and download the\\nones that you want.\\nMost accelerators include a Sample project with everything that you need to analyze data, build a\\nmodel, and display results. The sample projects include detailed instructions, data sets, Juptyer\\nnotebooks, models, and R Shiny applications. Use these sample projects as templates for your own\\ndata science needs to learn speciﬁc techniques, or to demonstrate the capabilities of Watson™ Studio\\nand other AI and analytics services.\\nMost accelerators also include a Business glossary that consists of terms and categories for data\\ngovernance. The terms and categories provide meaning to the accelerator and act as the information\\narchitecture for the accelerator.\\nSample project\\nA project contains the assets that you need to build and train the models that are associated with the\\naccelerator. You import the project with data science assets.\\nAudience\\nData scientists or business analysts who analyze data and build models to solve business\\nproblems.\\nContents\\nA readme file that provides instructions.\\nCSV files that contain the sample data.\\nPython notebooks to train and score the models and associated Python scripts to prepare and\\ntransform the data for modeling. The notebooks include API endpoints to expose the output for\\nthe R Shiny application.\\nMachine learning models that are designed to help you find answers to the business problems\\ndescribed by the accelerator.Sample project –\\nBusiness glossary –\\nCloud Pak for Data industry accelerators catalog –\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 1'})]}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"where are they located?\"\n",
    "response=conversation_chain({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])\n",
    "print(\"\\nConversation Chain: \\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e6f138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}),\n",
       " Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}),\n",
       " Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 3/3A set of business terms to describe data, with logical relationships between terms. See Business\\nterms.\\nA category with the same name as the accelerator in which to organize the terms. See\\nCategories.\\nRequirements\\nThe IBM® Knowledge Catalog service.\\nYou must have the Manage governance categories permission. To see which permissions you\\nhave, click your user avatar, select Profile and settings, and then view the Permissions page. If\\nyou need more permissions, contact your Cloud Pak for Data administrator.\\nProcess overview\\nEach accelerator provides detailed instructions that you can access after you download the\\naccelerator. These general steps provide an overview of the process:\\n1. Import the category.\\n2. Import the business terms.\\n3. Publish the business terms so that they are available in all catalogs and during automated\\ndiscovery.\\n4. Assign business terms to columns in data sets with one of these methods:\\nParent topic:\\nServices and integrationsAdd business terms to a data asset in a catalog on the asset's Overview page. See Editing\\nasset properties.▪\\nSome accelerators provide a Jupyter Notebook that assigns terms to the data sets included\\nin the sample project.▪\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 3'}),\n",
       " Document(page_content=\"01/01/2024, 17:50 Industry accelerators - IBM Documentation\\nhttps://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=integrations-industry-accelerators 1/3Industry accelerators\\nLast Updated: 2023-11-29\\nThe industry accelerators that are provided by IBM are a set of artifacts that help you address common\\nbusiness issues.\\nEach industry accelerator is designed to help you solve a speciﬁc business problem, whether it's\\npreventing credit card fraud in the banking industry or optimizing the efﬁciency of your contact center.\\nBrowse the Accelerators catalog for the Cloud Pak for Data industry accelerators and download the\\nones that you want.\\nMost accelerators include a Sample project with everything that you need to analyze data, build a\\nmodel, and display results. The sample projects include detailed instructions, data sets, Juptyer\\nnotebooks, models, and R Shiny applications. Use these sample projects as templates for your own\\ndata science needs to learn speciﬁc techniques, or to demonstrate the capabilities of Watson™ Studio\\nand other AI and analytics services.\\nMost accelerators also include a Business glossary that consists of terms and categories for data\\ngovernance. The terms and categories provide meaning to the accelerator and act as the information\\narchitecture for the accelerator.\\nSample project\\nA project contains the assets that you need to build and train the models that are associated with the\\naccelerator. You import the project with data science assets.\\nAudience\\nData scientists or business analysts who analyze data and build models to solve business\\nproblems.\\nContents\\nA readme file that provides instructions.\\nCSV files that contain the sample data.\\nPython notebooks to train and score the models and associated Python scripts to prepare and\\ntransform the data for modeling. The notebooks include API endpoints to expose the output for\\nthe R Shiny application.\\nMachine learning models that are designed to help you find answers to the business problems\\ndescribed by the accelerator.Sample project –\\nBusiness glossary –\\nCloud Pak for Data industry accelerators catalog –\", metadata={'title': 'Industry accelerators - IBM Documentation.pdf page 1'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f11a",
   "metadata": {},
   "source": [
    "### Detect and Solve Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c06c3",
   "metadata": {},
   "source": [
    "The response to the second question appears to be generated by the LLMs and not directly retrieved from the documents, resulting in an answer that seems out of context. To address such instances of misinformation or 'hallucination,' we previously developed the function `validate_answer_against_sources`. We can use this function to cross-check the answer with the source documents to ensure its accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "185c23b841a7cfae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:57.591045Z",
     "start_time": "2024-01-07T10:44:57.442686Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  who developed them?\n",
      "A:  Sorry I can not answer the question based on the given documents\n"
     ]
    }
   ],
   "source": [
    "if response['source_documents']:\n",
    "    response_answer = response['answer']\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "    is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "    if not is_valid_answer:\n",
    "        response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "else:\n",
    "    response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb64f65",
   "metadata": {},
   "source": [
    "We have now set up end to end Retrieval Augmented Generation Chatbot using Elastic Search, LangChain and WatsonX. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
